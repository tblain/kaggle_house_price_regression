{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels_and_features(dataset):\n",
    "    \"\"\"Extracts labels and features.\n",
    "\n",
    "    This is a good place to scale or transform the features if needed.\n",
    "\n",
    "    Args:\n",
    "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
    "    monochrome pixel values on the remaining columns, in row major order.\n",
    "    Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "    labels: A Pandas `Series`.\n",
    "    features: A Pandas `DataFrame`.\n",
    "    \"\"\"\n",
    "    labels = dataset[0]\n",
    "    # DataFrame.loc index ranges are inclusive at both ends.\n",
    "    features = dataset.loc[:,1:784]\n",
    "    # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "    features = features.div(255)\n",
    "\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35253    7\n",
       "41488    5\n",
       "31026    4\n",
       "14176    2\n",
       "27641    8\n",
       "        ..\n",
       "23642    0\n",
       "29882    2\n",
       "40746    1\n",
       "14217    3\n",
       "18396    8\n",
       "Name: 0, Length: 30000, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets, train_examples = parse_labels_and_features(df_mnist[1000:30000])\n",
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25698    6\n",
       "9710     1\n",
       "16969    5\n",
       "25721    0\n",
       "41415    2\n",
       "        ..\n",
       "5764     0\n",
       "22904    5\n",
       "29542    0\n",
       "10395    2\n",
       "37893    9\n",
       "Name: 0, Length: 12000, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets, test_examples = parse_labels_and_features(df_mnist[:1000])\n",
    "test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35253    7\n",
       "41488    5\n",
       "31026    4\n",
       "14176    2\n",
       "27641    8\n",
       "        ..\n",
       "7371     1\n",
       "5858     4\n",
       "5011     4\n",
       "10595    5\n",
       "9310     9\n",
       "Name: 0, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_targets, validation_examples = parse_labels_and_features(df_mnist[30000:])\n",
    "validation_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_example = np.random.choice(train_examples.index)\n",
    "_, ax = plt.subplots()\n",
    "ax.matshow(train_examples.loc[rand_example].values.reshape(28, 28))\n",
    "ax.set_title(\"Label: %i\" % train_targets.loc[rand_example])\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_2d(inputs, rotation=0, horizontal_flip=False, vertical_flip=False):\n",
    "    \"\"\"Apply additive augmentation on 2D data.\n",
    "\n",
    "    # Arguments\n",
    "      rotation: A float, the degree range for rotation (0 <= rotation < 180),\n",
    "          e.g. 3 for random image rotation between (-3.0, 3.0).\n",
    "      horizontal_flip: A boolean, whether to allow random horizontal flip,\n",
    "          e.g. true for 50% possibility to flip image horizontally.\n",
    "      vertical_flip: A boolean, whether to allow random vertical flip,\n",
    "          e.g. true for 50% possibility to flip image vertically.\n",
    "\n",
    "    # Returns\n",
    "      input data after augmentation, whose shape is the same as its original.\n",
    "    \"\"\"\n",
    "    if inputs.dtype != tf.float32:\n",
    "        inputs = tf.image.convert_image_dtype(inputs, dtype=tf.float32)\n",
    "\n",
    "    with tf.name_scope('augmentation'):\n",
    "        shp = tf.shape(inputs)\n",
    "        batch_size, height, width = shp[0], shp[1], shp[2]\n",
    "        width = tf.cast(width, tf.float32)\n",
    "        height = tf.cast(height, tf.float32)\n",
    "\n",
    "        transforms = []\n",
    "        identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)\n",
    "\n",
    "        if rotation > 0:\n",
    "            angle_rad = rotation * 3.141592653589793 / 180.0\n",
    "            angles = tf.random_uniform([batch_size], -angle_rad, angle_rad)\n",
    "            f = tf.contrib.image.angles_to_projective_transforms(angles,\n",
    "                                                                 height, width)\n",
    "            transforms.append(f)\n",
    "\n",
    "        if horizontal_flip:\n",
    "            coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n",
    "            shape = [-1., 0., width, 0., 1., 0., 0., 0.]\n",
    "            flip_transform = tf.convert_to_tensor(shape, dtype=tf.float32)\n",
    "            flip = tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1])\n",
    "            noflip = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])\n",
    "            transforms.append(tf.where(coin, flip, noflip))\n",
    "\n",
    "        if vertical_flip:\n",
    "            coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), 0.5)\n",
    "            shape = [1., 0., 0., 0., -1., height, 0., 0.]\n",
    "            flip_transform = tf.convert_to_tensor(shape, dtype=tf.float32)\n",
    "            flip = tf.tile(tf.expand_dims(flip_transform, 0), [batch_size, 1])\n",
    "            noflip = tf.tile(tf.expand_dims(identity, 0), [batch_size, 1])\n",
    "            transforms.append(tf.where(coin, flip, noflip))\n",
    "\n",
    "    if transforms:\n",
    "        f = tf.contrib.image.compose_transforms(*transforms)\n",
    "        inputs = tf.contrib.image.transform(inputs, f, interpolation='BILINEAR')\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, Lambda, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "import keras\n",
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = layers.Input(shape=(28, 28, 1))\n",
    "\n",
    "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.Conv2D(128, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "x = layers.Conv2D(256, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Flatten feature map to a 1-dim tensor so we can add fully connected layers\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "# Create output layer with a single node and sigmoid activation\n",
    "output = layers.Dense(10, activation='sigmoid')(x)\n",
    "\n",
    "# Create model:\n",
    "# input = input feature map\n",
    "# output = input feature map + stacked convolution/maxpooling layers + fully \n",
    "# connected layer + sigmoid output layer\n",
    "model = Model(img_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 2, 2, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 529,034\n",
      "Trainable params: 529,034\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "train = pd.read_csv(\"/home/tblain/Documents/projet_perso/kaggle_MNIST/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_org = np.array(train.drop(['label'], axis=1)).reshape((-1, 28, 28, 1))\n",
    "\n",
    "y_train_org = train.label\n",
    "y_train_org = to_categorical(y_train_org, num_classes=10)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_org, y_train_org, test_size = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1/255,\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "656/656 [==============================] - 29s 44ms/step - loss: 0.0258 - acc: 0.9925 - val_loss: nan - val_acc: 0.9571\n",
      "Epoch 2/10\n",
      "656/656 [==============================] - 29s 44ms/step - loss: 0.0254 - acc: 0.9928 - val_loss: nan - val_acc: 0.9500\n",
      "Epoch 3/10\n",
      "656/656 [==============================] - 29s 44ms/step - loss: 0.0248 - acc: 0.9929 - val_loss: nan - val_acc: 0.9738\n",
      "Epoch 4/10\n",
      "656/656 [==============================] - 29s 44ms/step - loss: 0.0223 - acc: 0.9933 - val_loss: nan - val_acc: 0.9738\n",
      "Epoch 5/10\n",
      "656/656 [==============================] - 29s 44ms/step - loss: 0.0242 - acc: 0.9934 - val_loss: nan - val_acc: 0.9786\n",
      "Epoch 6/10\n",
      "656/656 [==============================] - 29s 44ms/step - loss: 0.0239 - acc: 0.9929 - val_loss: nan - val_acc: 0.9738\n",
      "Epoch 7/10\n",
      "656/656 [==============================] - 32s 49ms/step - loss: 0.0240 - acc: 0.9933 - val_loss: nan - val_acc: 0.9595\n",
      "Epoch 8/10\n",
      "656/656 [==============================] - 30s 45ms/step - loss: 0.0216 - acc: 0.9936 - val_loss: nan - val_acc: 0.9643\n",
      "Epoch 9/10\n",
      "656/656 [==============================] - 27s 42ms/step - loss: 0.0257 - acc: 0.9928 - val_loss: nan - val_acc: 0.9167\n",
      "Epoch 10/10\n",
      "656/656 [==============================] - 27s 42ms/step - loss: 0.0225 - acc: 0.9929 - val_loss: nan - val_acc: 0.9524\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "data_flow = datagen.flow(x=X_train, y=y_train, batch_size=batch_size)\n",
    "history = model.fit_generator(data_flow, steps_per_epoch=len(X_train_org) // batch_size, epochs=10, \\\n",
    "                              validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(pd.read_csv(\"/home/tblain/Documents/projet_perso/kaggle_MNIST/test.csv\")).reshape((-1, 28, 28, 1))\n",
    "X_test = X_test / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 6s 197us/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict(X_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.argmax(y_test, axis=-1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(y_test, index=range(1, len(y_test)+1), columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(\"submission.csv\", index_label='ImageId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
